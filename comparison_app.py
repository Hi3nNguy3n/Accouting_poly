# Copyright by hiennm22, LocTH5 - BM UDPM
import streamlit as st
import pandas as pd
import os
import glob
import zipfile
import io
import re
import base64
import json
import os.path
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from email.utils import formataddr
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# C·ªë g·∫Øng import pypdf v√† h∆∞·ªõng d·∫´n c√†i ƒë·∫∑t n·∫øu thi·∫øu
try:
    import pypdf
except ImportError:
    st.error("Th∆∞ vi·ªán pypdf ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng ch·∫°y l·ªánh sau trong terminal: pip install pypdf")
    st.stop()

# --- H√ÄM H·ªñ TR·ª¢ OAUTH2 & ƒêƒÇNG NH·∫¨P ---
SCOPES = [
    'https://www.googleapis.com/auth/gmail.send',
    'https://www.googleapis.com/auth/gmail.readonly',
    'openid',
    'https://www.googleapis.com/auth/userinfo.email',
    'https://www.googleapis.com/auth/userinfo.profile'
]
TOKEN_FILE = "token.json"
CREDENTIALS_FILE = "credentials.json"

def get_google_credentials(credentials_json_content):
    """H√†m n√†y x·ª≠ l√Ω c·∫£ vi·ªác l·∫•y credentials v√† th√¥ng tin ng∆∞·ªùi d√πng."""
    creds = None
    # T·∫£i credentials t·ª´ session n·∫øu c√≥
    if 'credentials' in st.session_state:
        creds_json = json.loads(st.session_state['credentials'])
        creds = Credentials.from_authorized_user_info(creds_json, SCOPES)

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
                st.session_state['credentials'] = creds.to_json()
            except Exception as e:
                st.error(f"L·ªói khi l√†m m·ªõi token: {e}")
                # X√≥a token h·ªèng v√† y√™u c·∫ßu ƒëƒÉng nh·∫≠p l·∫°i
                if 'credentials' in st.session_state:
                    del st.session_state['credentials']
                if 'user_info' in st.session_state:
                    del st.session_state['user_info']
                return None, None
        else:
            # If there are no valid credentials, just return None.
            # The login page will handle the interactive login.
            return None, None

    # Sau khi c√≥ credentials, l·∫•y th√¥ng tin ng∆∞·ªùi d√πng
    try:
        service = build('oauth2', 'v2', credentials=creds)
        user_info = service.userinfo().get().execute()
        
        # --- KI·ªÇM TRA EMAIL ---
        email = user_info.get('email', '').lower()
        if not email.endswith('@fpt.edu.vn'):
            st.error("Truy c·∫≠p b·ªã t·ª´ ch·ªëi. Ch·ªâ c√°c t√†i kho·∫£n email FPT (@fpt.edu.vn) m·ªõi ƒë∆∞·ª£c ph√©p ƒëƒÉng nh·∫≠p.")
            # X√≥a th√¥ng tin ƒëƒÉng nh·∫≠p kh√¥ng h·ª£p l·ªá
            keys_to_delete = ['credentials', 'user_info']
            for key in keys_to_delete:
                if key in st.session_state:
                    del st.session_state[key]
            return None, None
        # --- K·∫æT TH√öC KI·ªÇM TRA EMAIL ---

        st.session_state['user_info'] = user_info
        return creds, user_info
    except HttpError as error:
        st.error(f"L·ªói khi l·∫•y th√¥ng tin ng∆∞·ªùi d√πng: {error}")
        return None, None

def show_login_page():
    """Hi·ªÉn th·ªã trang ƒëƒÉng nh·∫≠p v√† x·ª≠ l√Ω vi·ªác nh·∫•n n√∫t."""
    st.set_page_config(page_title="ƒêƒÉng nh·∫≠p", layout="centered", page_icon="üîë")
    st.title("üîë ƒêƒÉng nh·∫≠p ƒëi c·ª•c d√†ng!")
    st.write("Vui l√≤ng ƒëƒÉng nh·∫≠p b·∫±ng t√†i kho·∫£n Google @fpt.edu.vn c·ªßa b·∫°n ƒë·ªÉ ti·∫øp t·ª•c.")

    # T·∫£i credentials.json
    credentials_json_content = None
    if os.path.exists(CREDENTIALS_FILE):
        with open(CREDENTIALS_FILE, 'r', encoding='utf-8') as f:
            credentials_json_content = f.read()
    else:
        st.error(f"Kh√¥ng t√¨m th·∫•y file `{CREDENTIALS_FILE}`. Vui l√≤ng ƒë·∫£m b·∫£o file n√†y t·ªìn t·∫°i trong th∆∞ m·ª•c.")
        st.stop()

    try:
        # Use Flow for web applications, not InstalledAppFlow
        flow = Flow.from_client_config(
            json.loads(credentials_json_content),
            scopes=SCOPES,
            redirect_uri='https://mergegrab.streamlit.app/'  # Must match GCP Console
        )

        # Check if the user has been redirected back from Google
        query_params = st.query_params
        if 'code' in query_params:
            with st.spinner("ƒêang x√°c th·ª±c, vui l√≤ng ch·ªù..."):
                try:
                    # Exchange the code for credentials
                    flow.fetch_token(code=query_params.get('code'))
                    creds = flow.credentials
                    st.session_state['credentials'] = creds.to_json()

                    # Clear the query parameters from the URL
                    st.query_params.clear()

                    # Get user info and validate
                    service = build('oauth2', 'v2', credentials=creds)
                    user_info = service.userinfo().get().execute()
                    email = user_info.get('email', '').lower()

                    if not email.endswith('@fpt.edu.vn'):
                        st.error("Truy c·∫≠p b·ªã t·ª´ ch·ªëi. Ch·ªâ c√°c t√†i kho·∫£n email FPT (@fpt.edu.vn) m·ªõi ƒë∆∞·ª£c ph√©p ƒëƒÉng nh·∫≠p.")
                        # Clear invalid session
                        keys_to_delete = ['credentials', 'user_info']
                        for key in keys_to_delete:
                            if key in st.session_state:
                                del st.session_state[key]
                    else:
                        st.session_state['user_info'] = user_info
                        st.success(f"ƒêƒÉng nh·∫≠p th√†nh c√¥ng! Xin ch√†o, {user_info.get('name', 'b·∫°n')}.")
                        # Rerun to show the main app
                        st.rerun()

                except Exception as e:
                    st.error(f"L·ªói khi x√°c th·ª±c v·ªõi Google: {e}")
        else:
            # Show the login button
            auth_url, _ = flow.authorization_url(prompt='consent')
            st.link_button("ƒêƒÉng nh·∫≠p v·ªõi Google", auth_url, use_container_width=True, help="B·∫°n s·∫Ω ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn trang ƒëƒÉng nh·∫≠p c·ªßa Google")

    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o quy tr√¨nh x√°c th·ª±c: {e}")
        st.exception(e)

def main_app():
    """H√†m ch·ª©a to√†n b·ªô giao di·ªán v√† logic c·ªßa ·ª©ng d·ª•ng ch√≠nh."""
    st.set_page_config(page_title="ƒê·ªëi chi·∫øu FPT", layout="wide", page_icon="üìä")

    # --- SIDEBAR & ƒêƒÇNG XU·∫§T ---
    with st.sidebar:
        user_info = st.session_state.get('user_info', {})
        st.markdown(f"Xin ch√†o, **{user_info.get('name', 'ng∆∞·ªùi d√πng')}**")
        st.caption(user_info.get('email'))
        if st.button("ƒêƒÉng xu·∫•t"):
            # X√≥a th√¥ng tin ƒëƒÉng nh·∫≠p kh·ªèi session
            keys_to_delete = ['credentials', 'user_info']
            for key in keys_to_delete:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

    st.title("üìä ƒê·ªëi chi·∫øu d·ªØ li·ªáu Grab & B√°o c√°o PDF")
    st.write("T·∫£i l√™n c√°c t·ªáp c·ªßa b·∫°n ƒë·ªÉ b·∫Øt ƒë·∫ßu ƒë·ªëi chi·∫øu v√† x·ª≠ l√Ω.")
    st.caption("Copyright by LocTH5, Hiennm22 - BM UDPM")

    # (D√°n to√†n b·ªô ph·∫ßn c√≤n l·∫°i c·ªßa ·ª©ng d·ª•ng v√†o ƒë√¢y)
    # --- GIAO DI·ªÜN NH·∫¨P LI·ªÜU ---
    with st.container(border=True):
        st.subheader("T·∫£i l√™n c√°c file c·∫ßn thi·∫øt")
        col1, col2, col3 = st.columns(3)
        file_types = ["csv", "xls", "xlsx"]
        with col1:
            uploaded_transport_file = st.file_uploader("1. File Transport", type=file_types)
            uploaded_express_file = st.file_uploader("2. File Express", type=file_types)
        with col2:
            uploaded_invoice_file = st.file_uploader("3. File H√≥a ƒë∆°n", type=file_types)
            uploaded_zip_file = st.file_uploader("4. Folder B√°o c√°o (.zip)", type=["zip"])
        with col3:
            uploaded_xml_zip_file = st.file_uploader("5. Folder XML (.zip)", type=["zip"])

    # --- C·∫§U H√åNH OAUTH 2.0 ---
    # The app now automatically loads 'credentials.json' from the local directory.
    CREDENTIALS_FILE = "credentials.json"
    st.session_state.credentials_loaded = False
    if os.path.exists(CREDENTIALS_FILE):
        with open(CREDENTIALS_FILE, 'r', encoding='utf-8') as f:
            st.session_state.credentials_json_content = f.read()
        st.session_state.credentials_loaded = True

    # --- B·∫ÆT ƒê·∫¶U X·ª¨ L√ù KHI C√ì ƒê·ª¶ FILE ---
    if (uploaded_transport_file is not None or uploaded_express_file is not None) and uploaded_invoice_file is not None:
        try:
            employee_to_unit_map, unit_to_email_map = load_mapping_data()

            # --- 1. ƒê·ªåC V√Ä L√ÄM S·∫†CH D·ªÆ LI·ªÜU G·ªêC ---
            source_dfs = []
            if uploaded_transport_file:
                try:
                    if uploaded_transport_file.name.endswith('.csv'):
                        df_transport_single = pd.read_csv(uploaded_transport_file, skiprows=7)
                    else:
                        df_transport_single = pd.read_excel(uploaded_transport_file, skiprows=7)
                    
                    if df_transport_single.shape[1] > 10:
                        # Rename Booking ID from Column K (index 10)
                        df_transport_single.rename(columns={df_transport_single.columns[10]: 'Booking ID'}, inplace=True)
                        # Rename Employee Name from Column C (index 2)
                        df_transport_single.rename(columns={df_transport_single.columns[2]: 'Employee Name'}, inplace=True)
                        source_dfs.append(df_transport_single)
                    else:
                        st.warning(f"File Transport '{uploaded_transport_file.name}' d∆∞·ªùng nh∆∞ kh√¥ng h·ª£p l·ªá (c·∫ßn √≠t nh·∫•t 11 c·ªôt). B·ªè qua file n√†y.")
                except Exception as e:
                    st.error(f"L·ªói khi ƒë·ªçc file Transport '{uploaded_transport_file.name}': {e}")

            if uploaded_express_file:
                try:
                    if uploaded_express_file.name.endswith('.csv'):
                        df_express_single = pd.read_csv(uploaded_express_file, skiprows=7)
                    else:
                        df_express_single = pd.read_excel(uploaded_express_file, skiprows=7)

                    if df_express_single.shape[1] > 9:
                        # Rename Booking ID from Column J (index 9)
                        df_express_single.rename(columns={df_express_single.columns[9]: 'Booking ID'}, inplace=True)
                        # Rename Employee Name from Column C (index 2)
                        df_express_single.rename(columns={df_express_single.columns[2]: 'Employee Name'}, inplace=True)
                        source_dfs.append(df_express_single)
                    else:
                        st.warning(f"File Express '{uploaded_express_file.name}' d∆∞·ªùng nh∆∞ kh√¥ng h·ª£p l·ªá (c·∫ßn √≠t nh·∫•t 10 c·ªôt). B·ªè qua file n√†y.")
                except Exception as e:
                    st.error(f"L·ªói khi ƒë·ªçc file Express '{uploaded_express_file.name}': {e}")

            if not source_dfs:
                st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω file Transport ho·∫∑c Express. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªãnh d·∫°ng file.")
                st.stop()
            
            df_transport = pd.concat(source_dfs, ignore_index=True)

            if uploaded_invoice_file.name.endswith('.csv'):
                df_invoice = pd.read_csv(uploaded_invoice_file)
            elif uploaded_invoice_file.name.endswith('.xls'):
                try:
                    df_invoice = pd.read_html(uploaded_invoice_file)[0]
                except Exception:
                    df_invoice = pd.read_excel(uploaded_invoice_file, engine='xlrd')
            else:
                df_invoice = pd.read_excel(uploaded_invoice_file)

            df_transport.columns = df_transport.columns.str.strip()
            df_invoice.columns = df_invoice.columns.str.strip()

            if len(df_transport.columns) > 9:
                pickup_col_name = df_transport.columns[7]
                dropoff_col_name = df_transport.columns[9]
                df_transport.rename(columns={
                    pickup_col_name: 'GEMINI_PICKUP_ADDRESS',
                    dropoff_col_name: 'GEMINI_DROPOFF_ADDRESS'
                }, inplace=True)

            if df_invoice.shape[1] < 13:
                st.error(f"File H√≥a ƒë∆°n kh√¥ng c√≥ ƒë·ªß 13 c·ªôt. Ch·ªâ t√¨m th·∫•y {df_invoice.shape[1]} c·ªôt.")
                st.stop()
            rename_dict = {
                df_invoice.columns[1]: 'pdf_link_key',
                df_invoice.columns[12]: 'summary_ma_nhan_hoa_don'
            }
            if len(df_invoice.columns) > 4:
                rename_dict[df_invoice.columns[4]] = 'GEMINI_NGAY_HD_INVOICE'
                rename_dict[df_invoice.columns[5]] = 'HINH_THUC_TT'
                rename_dict[df_invoice.columns[6]] = 'TIEN_TRC_THUE'
                rename_dict[df_invoice.columns[7]] = 'TIEN_THUE8'
                rename_dict[df_invoice.columns[8]] = 'TONG_TIEN'
                rename_dict[df_invoice.columns[15]] = 'NGAY_BOOKING'
                rename_dict[df_invoice.columns[16]] = 'SO_HOA_DON'
            df_invoice.rename(columns=rename_dict, inplace=True)

            # Check for unmatched Booking IDs between Transport and Invoice files
            transport_ids = set(df_transport['Booking ID'].dropna())
            invoice_ids = set(df_invoice['Booking'].dropna())

            unmatched_transport_ids = transport_ids - invoice_ids
            unmatched_invoice_ids = invoice_ids - transport_ids

            if unmatched_transport_ids:
                st.warning(f"T√¨m th·∫•y {len(unmatched_transport_ids)} Booking ID ch·ªâ c√≥ trong file Transport (kh√¥ng c√≥ trong file H√≥a ƒë∆°n):")
                with st.expander("Xem danh s√°ch ID b·ªã th·ª´a t·ª´ file Transport"):
                    st.dataframe(pd.DataFrame(sorted(list(unmatched_transport_ids)), columns=["Booking ID"]))

            if unmatched_invoice_ids:
                st.warning(f"T√¨m th·∫•y {len(unmatched_invoice_ids)} Booking ID ch·ªâ c√≥ trong file H√≥a ƒë∆°n (kh√¥ng c√≥ trong file Transport):")
                with st.expander("Xem danh s√°ch ID b·ªã th·ª´a t·ª´ file H√≥a ƒë∆°n"):
                    st.dataframe(pd.DataFrame(sorted(list(unmatched_invoice_ids)), columns=["Booking ID"]))


            matching_ids = list(set(df_transport['Booking ID'].dropna()) & set(df_invoice['Booking'].dropna()))
            if not matching_ids:
                st.warning("Kh√¥ng t√¨m th·∫•y Booking ID n√†o tr√πng kh·ªõp gi·ªØa hai file ƒë·∫ßu v√†o.")
                st.stop()

            df_merged = pd.merge(df_transport[df_transport['Booking ID'].isin(matching_ids)], df_invoice[df_invoice['Booking'].isin(matching_ids)], left_on='Booking ID', right_on='Booking', suffixes=('_transport', '_invoice'))
            df_merged['ƒê∆°n v·ªã'] = df_merged['Employee Name'].map(employee_to_unit_map)
            df_merged['ƒê∆°n v·ªã'].fillna('Kh√¥ng x√°c ƒë·ªãnh', inplace=True)

            # --- 3. X·ª¨ L√ù FOLDER PDF (N·∫æU C√ì) ---
            count_no_pdf = 0
            if uploaded_zip_file is not None:
                pdf_data = []
                with zipfile.ZipFile(uploaded_zip_file, 'r') as zip_ref:
                    pdf_file_names = [name for name in zip_ref.namelist() if name.lower().endswith('.pdf') and not name.startswith('__MACOSX')]
                    st.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(pdf_file_names)} file PDF t·ª´ t·ªáp .zip...")
                    progress_bar = st.progress(0, text="ƒêang x·ª≠ l√Ω file PDF...")
                    for i, filename in enumerate(pdf_file_names):
                        try:
                            key_from_filename = os.path.basename(filename).split('_')[2]
                            with zip_ref.open(filename) as pdf_file:
                                pdf_content = pdf_file.read()
                                pdf_reader = pypdf.PdfReader(io.BytesIO(pdf_content))
                                text = "".join([page.extract_text() or "" for page in pdf_reader.pages])
                                found_code = "Kh√¥ng t√¨m th·∫•y trong PDF"
                                if "M√£ nh·∫≠n h√≥a ƒë∆°n" in text:
                                    parts = text.split("M√£ nh·∫≠n h√≥a ƒë∆°n")
                                    if len(parts) > 1:
                                        code = parts[1].split('\n')[0].replace(":", "").strip()
                                        if code: found_code = code
                                ngay_hd_str = "Kh√¥ng t√¨m th·∫•y"
                                match = re.search(r'Ng√†y\s*(\d{1,2})\s*th√°ng\s*(\d{1,2})\s*nƒÉm\s*(\d{4})', text, re.IGNORECASE)
                                if match:
                                    day, month, year = match.groups()
                                    ngay_hd_str = f"{day.zfill(2)}/{month.zfill(2)}/{year}"
                                pdf_data.append({'pdf_link_key_str': key_from_filename, 'M√£ h√≥a ƒë∆°n t·ª´ PDF': found_code, 'Ngay_HD_pdf': ngay_hd_str, 'pdf_content': pdf_content, 'pdf_filename': os.path.basename(filename)})
                        except Exception as e:
                            st.warning(f"L·ªói khi ƒë·ªçc file {filename} trong zip: {e}")
                        progress_bar.progress((i + 1) / len(pdf_file_names), text=f"ƒêang x·ª≠ l√Ω: {os.path.basename(filename)}")
                if pdf_data:
                    df_pdf_data = pd.DataFrame(pdf_data)
                    df_merged['pdf_link_key_str'] = df_merged['pdf_link_key'].astype(str)

                    # Check for PDF files that don't match any invoice row
                    merged_keys = set(df_merged['pdf_link_key_str'])
                    pdf_keys = set(df_pdf_data['pdf_link_key_str'])
                    unmatched_pdf_keys = pdf_keys - merged_keys

                    if unmatched_pdf_keys:
                        unmatched_pdf_files = df_pdf_data[df_pdf_data['pdf_link_key_str'].isin(unmatched_pdf_keys)]
                        st.warning(f"T√¨m th·∫•y {len(unmatched_pdf_files)} file PDF kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ d√≤ng n√†o trong file H√≥a ƒë∆°n:")
                        with st.expander("Xem danh s√°ch file PDF b·ªã th·ª´a"):
                            st.dataframe(unmatched_pdf_files[['pdf_filename', 'pdf_link_key_str']])

                    df_merged = pd.merge(df_merged, df_pdf_data, on='pdf_link_key_str', how='left')
                    count_no_pdf = df_merged['pdf_filename'].isnull().sum()

            # --- 3.1. X·ª¨ L√ù FOLDER XML (N·∫æU C√ì) ---
            if uploaded_xml_zip_file is not None:
                xml_data = []
                with zipfile.ZipFile(uploaded_xml_zip_file, 'r') as zip_ref:
                    xml_file_names = [name for name in zip_ref.namelist() if name.lower().endswith('.xml') and not name.startswith('__MACOSX')]
                    st.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(xml_file_names)} file XML t·ª´ t·ªáp .zip...")
                    progress_bar_xml = st.progress(0, text="ƒêang x·ª≠ l√Ω file XML...")
                    for i, filename in enumerate(xml_file_names):
                        try:
                            base = os.path.basename(filename)
                            key_from_filename = ''
                            try:
                                # Attempt 1: PDF-style naming (e.g., 1_C25MGA_2565515_....xml)
                                key_from_filename = base.split('_')[2]
                            except IndexError:
                                # Attempt 2: Simple naming (e.g., 2565515.xml)
                                key_from_filename = base.split('.')[0]

                            if not key_from_filename.strip():
                                st.warning(f"Kh√¥ng th·ªÉ l·∫•y key t·ª´ t√™n file XML: '{filename}'. B·ªè qua file n√†y.")
                                continue

                            with zip_ref.open(filename) as xml_file:
                                xml_content = xml_file.read()
                                text = xml_content.decode('utf-8', errors='ignore')

                                # Extract invoice code from XML using regex on common tags
                                found_code = "Kh√¥ng t√¨m th·∫•y trong XML"
                                code_match = re.search(r'<InvoiceCode>(.*?)</InvoiceCode>', text, re.IGNORECASE) or \
                                             re.search(r'<MaNhanHoaDon>(.*?)</MaNhanHoaDon>', text, re.IGNORECASE) or \
                                             re.search(r'<TransactionID>(.*?)</TransactionID>', text, re.IGNORECASE) or \
                                             re.search(r'<Fkey>(.*?)</Fkey>', text, re.IGNORECASE) # Another common one
                                if code_match:
                                    found_code = code_match.group(1).strip()

                                # Extract invoice date from XML using regex
                                ngay_hd_str = "Kh√¥ng t√¨m th·∫•y"
                                date_match = re.search(r'<IssuedDate>(.*?)</IssuedDate>', text, re.IGNORECASE) # YYYY-MM-DD
                                if date_match:
                                    try:
                                        # Handle various date formats that might be in the tag
                                        dt = pd.to_datetime(date_match.group(1))
                                        ngay_hd_str = dt.strftime('%d/%m/%Y')
                                    except Exception:
                                        # If parsing fails, try to use the raw value
                                        ngay_hd_str = date_match.group(1).strip()
                                else:
                                    # Fallback to the same regex as for PDFs
                                    match = re.search(r'Ng√†y\s*(\d{1,2})\s*th√°ng\s*(\d{1,2})\s*nƒÉm\s*(\d{4})', text, re.IGNORECASE)
                                    if match:
                                        day, month, year = match.groups()
                                        ngay_hd_str = f"{day.zfill(2)}/{month.zfill(2)}/{year}"

                                xml_data.append({
                                    'pdf_link_key_str': key_from_filename,
                                    'M√£ h√≥a ƒë∆°n t·ª´ XML': found_code,
                                    'Ngay_HD_xml': ngay_hd_str,
                                    'xml_content': xml_content,
                                    'xml_filename': os.path.basename(filename)
                                })
                        except Exception as e:
                            st.warning(f"L·ªói khi ƒë·ªçc file XML {filename} trong zip: {e}")
                        progress_bar_xml.progress((i + 1) / len(xml_file_names), text=f"ƒêang x·ª≠ l√Ω: {os.path.basename(filename)}")
                
                if xml_data:
                    df_xml_data = pd.DataFrame(xml_data)
                    if 'pdf_link_key_str' not in df_merged.columns:
                        df_merged['pdf_link_key_str'] = df_merged['pdf_link_key'].astype(str)
                    
                    # Check for XML files that don't match any invoice row
                    merged_keys_for_xml = set(df_merged['pdf_link_key_str'])
                    xml_keys = set(df_xml_data['pdf_link_key_str'])
                    unmatched_xml_keys = xml_keys - merged_keys_for_xml

                    if unmatched_xml_keys:
                        unmatched_xml_files = df_xml_data[df_xml_data['pdf_link_key_str'].isin(unmatched_xml_keys)]
                        st.warning(f"T√¨m th·∫•y {len(unmatched_xml_files)} file XML kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ d√≤ng n√†o trong file H√≥a ƒë∆°n:")
                        with st.expander("Xem danh s√°ch file XML b·ªã th·ª´a"):
                            st.dataframe(unmatched_xml_files[['xml_filename', 'pdf_link_key_str']])

                    df_merged = pd.merge(df_merged, df_xml_data, on='pdf_link_key_str', how='left')

            # --- 4. TH·ªêNG K√ä V√Ä HI·ªÇN TH·ªä ---
            if count_no_pdf > 0:
                st.warning(f"### ‚ö†Ô∏è Ch√∫ √Ω: C√≥ {count_no_pdf} h√≥a ƒë∆°n kh√¥ng c√≥ file PDF t∆∞∆°ng ·ª©ng.")
                with st.expander("Xem danh s√°ch v√† th·ªëng k√™ c√°c h√≥a ƒë∆°n thi·∫øu PDF"):
                    df_missing_pdfs = df_merged[df_merged['pdf_filename'].isnull()]
                    
                    # 1. Display statistics table
                    st.markdown("#### Th·ªëng k√™ theo ƒê∆°n v·ªã")
                    missing_stats = df_missing_pdfs.groupby('ƒê∆°n v·ªã').agg(
                        so_hoa_don_thieu=('Booking ID', 'count'),
                        tong_tien_thieu=('Total Fare', 'sum')
                    ).reset_index()
                    missing_stats.rename(columns={
                        'so_hoa_don_thieu': 'S·ªë h√≥a ƒë∆°n thi·∫øu PDF',
                        'tong_tien_thieu': 'T·ªïng ti·ªÅn (∆∞·ªõc t√≠nh)'
                    }, inplace=True)
                    st.dataframe(missing_stats)

                    # 2. Display the raw list
                    st.markdown("---")
                    st.markdown("#### Danh s√°ch chi ti·∫øt")
                    
                    cols_to_show = ['Booking ID', 'Employee Name', 'ƒê∆°n v·ªã', 'pdf_link_key', 'Total Fare']
                    date_cols = ['Date', 'Date of Trip', 'Trip Date', 'Ng√†y', 'Date & Time (GMT+7)']
                    available_date_col = find_col(df_missing_pdfs, date_cols)
                    if available_date_col:
                        cols_to_show.append(available_date_col)

                    existing_cols_to_show = [col for col in cols_to_show if col in df_missing_pdfs.columns]
                    st.dataframe(df_missing_pdfs[existing_cols_to_show])

            st.header("K·∫øt qu·∫£ ƒë·ªëi chi·∫øu")

            with st.container(border=True):
                st.subheader("B·∫£ng th·ªëng k√™ t·ªïng h·ª£p")

                if 'Employee Name' not in df_merged.columns:
                    st.error("Kh√¥ng th·ªÉ t·∫°o b·∫£ng th·ªëng k√™: Thi·∫øu c·ªôt 'Employee Name' trong d·ªØ li·ªáu ƒë√£ h·ª£p nh·∫•t.")
                else:
                    agg_dict = {}
                    if 'Booking ID' in df_merged.columns:
                        agg_dict['So chuyen'] = ('Booking ID', 'count')
                    if 'Total Fare' in df_merged.columns:
                        agg_dict['Tong tien (VND)'] = ('Total Fare', 'sum')

                    if not agg_dict:
                        st.warning("Kh√¥ng th·ªÉ t·∫°o b·∫£ng th·ªëng k√™: Thi·∫øu c·∫£ c·ªôt 'Booking ID' v√† 'Total Fare'.")
                    else:
                        summary_df = df_merged.groupby('Employee Name').agg(**agg_dict).reset_index()

                        if 'Tong tien (VND)' in summary_df.columns:
                            summary_df = summary_df.sort_values('Tong tien (VND)', ascending=False).reset_index(drop=True)
                        elif 'So chuyen' in summary_df.columns:
                            summary_df = summary_df.sort_values('So chuyen', ascending=False).reset_index(drop=True)

                        if summary_df.empty:
                            st.info("Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ th·ªëng k√™.")
                        else:
                            display_cols_spec = {"T√™n Ng∆∞·ªùi d√πng": 3}
                            if 'So chuyen' in summary_df.columns:
                                display_cols_spec["S·ªë chuy·∫øn"] = 1
                            if 'Tong tien (VND)' in summary_df.columns:
                                display_cols_spec["T·ªïng ti·ªÅn (VND)"] = 2

                            header_cols = st.columns(list(display_cols_spec.values()))
                            for i, col_name in enumerate(display_cols_spec.keys()):
                                header_cols[i].markdown(f"**{col_name}**")

                            current_employees = summary_df['Employee Name'].tolist()
                            expanded_state = st.session_state.setdefault("expanded_employees", {})
                            stale_keys = [name for name in expanded_state.keys() if name not in current_employees]
                            for name in stale_keys:
                                del expanded_state[name]
                            for name in current_employees:
                                expanded_state.setdefault(name, False)

                            def toggle_employee_expansion(employee_name: str) -> None:
                                state = st.session_state["expanded_employees"]
                                state[employee_name] = not state.get(employee_name, False)

                            date_cols = ['Date', 'Date of Trip', 'Trip Date', 'Ng√†y', 'Date & Time (GMT+7)']
                            date_col_name = find_col(df_merged, date_cols)
                            pickup_col_name = 'GEMINI_PICKUP_ADDRESS' if 'GEMINI_PICKUP_ADDRESS' in df_merged.columns else None
                            dropoff_col_name = 'GEMINI_DROPOFF_ADDRESS' if 'GEMINI_DROPOFF_ADDRESS' in df_merged.columns else None

                            for idx, row in summary_df.iterrows():
                                employee_name = row['Employee Name']
                                expanded = st.session_state["expanded_employees"][employee_name]
                                row_container = st.container()
                                with row_container:
                                    row_cols = st.columns(list(display_cols_spec.values()))
                                    with row_cols[0]:
                                        label = f"{'‚ñº' if expanded else '‚ñ∫'} {employee_name}"
                                        st.button(
                                            label,
                                            key=f"summary_row_{idx}",
                                            use_container_width=True,
                                            on_click=toggle_employee_expansion,
                                            args=(employee_name,),
                                        )
                                    
                                    current_col_index = 1
                                    if 'So chuyen' in summary_df.columns:
                                        with row_cols[current_col_index]:
                                            st.write(int(row['So chuyen']))
                                        current_col_index += 1
                                    if 'Tong tien (VND)' in summary_df.columns:
                                        with row_cols[current_col_index]:
                                            st.write(f"{row['Tong tien (VND)']:,.0f}")

                                if st.session_state["expanded_employees"].get(employee_name):
                                    employee_df = df_merged[df_merged['Employee Name'] == employee_name]
                                    usage_date_col = 'NGAY_BOOKING' if 'NGAY_BOOKING' in employee_df.columns else date_col_name
                                    detail_cols = [
                                        'Employee Name', 'Booking ID', pickup_col_name, dropoff_col_name,
                                        'GEMINI_NGAY_HD_INVOICE', 'HINH_THUC_TT', 'TIEN_TRC_THUE',
                                        'TIEN_THUE8', 'TONG_TIEN', usage_date_col, 'SO_HOA_DON',
                                    ]
                                    final_cols = [col for col in detail_cols if col and col in employee_df.columns]

                                    detail_df = employee_df[final_cols].copy()
                                    detail_df.insert(0, "STT", range(1, len(detail_df) + 1))
                                    if 'SO_HOA_DON' in detail_df.columns:
                                        detail_df['SO_HOA_DON'] = detail_df['SO_HOA_DON'].astype(str).str.split('_').str[0].replace('nan', '')

                                    rename_map = {
                                        'Employee Name': 'Ng∆∞·ªùi s·ª≠ d·ª•ng', 'Booking ID': 'M√£ ƒë·∫∑t ch·ªó',
                                        'GEMINI_PICKUP_ADDRESS': 'ƒêi·ªÉm ƒë√≥n', 'GEMINI_DROPOFF_ADDRESS': 'ƒêi·ªÉm ƒë·∫øn',
                                        'GEMINI_NGAY_HD_INVOICE': 'Ng√†y Hƒê', 'HINH_THUC_TT': 'H√¨nh th·ª©c thanh to√°n',
                                        'TIEN_TRC_THUE': 'T·ªïng ti·ªÅn tr∆∞·ªõc thu·∫ø', 'TIEN_THUE8': 'T·ªïng ti·ªÅn thu·∫ø (8%)',
                                        'TONG_TIEN': 'T·ªïng ti·ªÅn ƒë√£ c√≥ thu·∫ø', 'NGAY_BOOKING': 'Ng√†y s·ª≠ d·ª•ng',
                                        'SO_HOA_DON': 'S·ªë h√≥a ƒë∆°n',
                                    }
                                    if usage_date_col and usage_date_col not in rename_map:
                                        rename_map[usage_date_col] = 'Ng√†y s·ª≠ d·ª•ng'
                                    detail_df.rename(columns={k: v for k, v in rename_map.items() if k in detail_df.columns}, inplace=True)

                                    employee_display_col = rename_map.get('Employee Name', 'Employee Name')
                                    money_cols = [
                                        rename_map.get('TIEN_TRC_THUE', 'TIEN_TRC_THUE'),
                                        rename_map.get('TIEN_THUE8', 'TIEN_THUE8'),
                                        rename_map.get('TONG_TIEN', 'TONG_TIEN'),
                                    ]
                                    
                                    # Only add total row if there are money columns to sum
                                    if any(col in detail_df.columns for col in money_cols):
                                        total_row = {col: "" for col in detail_df.columns}
                                        total_row['STT'] = ""
                                        if employee_display_col in total_row:
                                            total_row[employee_display_col] = 'T·ªïng c·ªông'
                                        for money_col in money_cols:
                                            if money_col in detail_df.columns:
                                                numeric_values = pd.to_numeric(detail_df[money_col], errors='coerce')
                                                total_row[money_col] = numeric_values.sum()
                                        detail_df = pd.concat([detail_df, pd.DataFrame([total_row])], ignore_index=True)

                                    for money_col in money_cols:
                                        if money_col in detail_df.columns:
                                            numeric_series = pd.to_numeric(detail_df[money_col], errors='coerce')
                                            detail_df[money_col] = numeric_series.apply(
                                                lambda value: f"{value:,.0f}" if pd.notna(value) else ""
                                            )

                                    st.dataframe(detail_df, use_container_width=True, hide_index=True)

                                st.markdown('<hr style="margin-top:0.25rem; margin-bottom:0.25rem;">', unsafe_allow_html=True)
                try:
                    date_cols = ['Date', 'Date of Trip', 'Trip Date', 'Ng√†y', 'Date & Time (GMT+7)']
                    date_col_name = find_col(df_merged, date_cols)
                    pickup_col_name = 'GEMINI_PICKUP_ADDRESS' if 'GEMINI_PICKUP_ADDRESS' in df_merged.columns else None
                    dropoff_col_name = 'GEMINI_DROPOFF_ADDRESS' if 'GEMINI_DROPOFF_ADDRESS' in df_merged.columns else None
        
                    if date_col_name is None:
                        try:
                            uploaded_transport_file.seek(0)
                            header_list = pd.read_csv(uploaded_transport_file, skiprows=8, nrows=1).columns.tolist() if uploaded_transport_file.name.endswith('.csv') else pd.read_excel(uploaded_transport_file, skiprows=8, nrows=1).columns.tolist()
                            st.error(f"**L·ªói B·∫£ng k√™: Kh√¥ng th·ªÉ t√¨m th·∫•y c·ªôt Ng√†y th√°ng.** Vui l√≤ng cho t√¥i bi·∫øt t√™n c·ªôt ng√†y th√°ng ch√≠nh x√°c t·ª´ danh s√°ch b√™n d∆∞·ªõi. **C√°c c·ªôt t√¨m ƒë∆∞·ª£c:** `{header_list}`")
                        except Exception as e:
                            st.error(f"L·ªói B·∫£ng k√™: Kh√¥ng t√¨m th·∫•y c·ªôt ng√†y th√°ng. L·ªói khi ƒë·ªçc c·ªôt: {e}")
                    else:
                        df_merged['Date_dt'] = pd.to_datetime(df_merged[date_col_name], errors='coerce')
                        start_date = df_merged['Date_dt'].min()
                        end_date = df_merged['Date_dt'].max()
        
                        @st.cache_data
                        def generate_bang_ke_excel(_df, s_date, e_date, _d_col, _p_col, _do_col):
                            from openpyxl import load_workbook
                            from openpyxl.styles import Font
                            template_path = "FileMau/BangKe.xlsx"
                            wb = load_workbook(template_path)
                            ws = wb.active
                            ws['C4'] = s_date.strftime('%d/%m/%Y') if pd.notna(s_date) else "N/A"
                            ws['C5'] = e_date.strftime('%d/%m/%Y') if pd.notna(e_date) else "N/A"
                            start_row = 8
                            for i, row in _df.reset_index(drop=True).iterrows():
                                ws.cell(row=start_row + i, column=1, value=i + 1)
                                ws.cell(row=start_row + i, column=2, value=row['Booking ID'])
                                ws.cell(row=start_row + i, column=3, value=f" {row['Employee Name']}")
                                if _p_col: ws.cell(row=start_row + i, column=4, value=row[_p_col])
                                if _do_col: ws.cell(row=start_row + i, column=5, value=row[_do_col])
                                if 'GEMINI_NGAY_HD_INVOICE' in row and pd.notna(row['GEMINI_NGAY_HD_INVOICE']): ws.cell(row=start_row + i, column=6, value=row['GEMINI_NGAY_HD_INVOICE'])
                                ws.cell(row=start_row + i, column=7, value=row['HINH_THUC_TT'])
                                ws.cell(row=start_row + i, column=8, value="{:,.0f}".format(row['TIEN_TRC_THUE']))
                                ws.cell(row=start_row + i, column=9, value="{:,.0f}".format(row['TIEN_THUE8']))
                                ws.cell(row=start_row + i, column=10, value="{:,.0f}".format(row['TONG_TIEN']))
                                ws.cell(row=start_row + i, column=11, value=row['NGAY_BOOKING'])
                                ws.cell(row=start_row + i, column=12, value=row['pdf_link_key'])
                            total_row_index = start_row + len(_df)
                            total_label_cell = ws.cell(row=total_row_index, column=7, value="T·ªïng c·ªông"); total_label_cell.font = Font(bold=True)
                            total_value_cell_8 = ws.cell(row=total_row_index, column=8, value="{:,.0f}".format(_df['TIEN_TRC_THUE'].sum())); total_value_cell_8.font = Font(bold=True)
                            total_value_cell_9 = ws.cell(row=total_row_index, column=9, value="{:,.0f}".format(_df['TIEN_THUE8'].sum())); total_value_cell_9.font = Font(bold=True)
                            total_value_cell_10 = ws.cell(row=total_row_index, column=10, value="{:,.0f}".format(_df['TONG_TIEN'].sum())); total_value_cell_10.font = Font(bold=True)
                            excel_buffer = io.BytesIO(); wb.save(excel_buffer); return excel_buffer.getvalue()
        
                        st.subheader("üìß G·ª≠i B·∫£ng K√™ qua Email (b·∫±ng Gmail)")
        
                        uploaded_email_mapping_file = st.file_uploader(
                            "T·∫£i file Email Mapping (b·∫Øt bu·ªôc ƒë·ªÉ g·ª≠i mail)",
                            type=["xlsx", "xls"],
                            help="T·∫£i l√™n file Excel ch·ª©a c·ªôt 'ƒê∆°n v·ªã' v√† 'Email' ƒë·ªÉ g·ª≠i b·∫£ng k√™."
                        )
        
                        # Read the mapping file as soon as it's uploaded
                        unit_to_email_map_upload = None
                        if uploaded_email_mapping_file is not None:
                            try:
                                uploaded_email_mapping_file.seek(0)
                                df_email_map_upload = pd.read_excel(uploaded_email_mapping_file)
                                email_col_upload = df_email_map_upload.columns[3]
                                unit_col_upload = df_email_map_upload.columns[4]
                                df_email_map_upload = df_email_map_upload.dropna(subset=[email_col_upload, unit_col_upload])
                                unit_to_email_map_upload = df_email_map_upload.groupby(unit_col_upload)[email_col_upload].apply(lambda x: list(x.unique())).to_dict()
                            except Exception as e:
                                st.error(f"L·ªói khi ƒë·ªçc file Email Mapping: {e}")
                                # Leave map as None and the error will be handled below
        
                        with st.expander("H∆∞·ªõng d·∫´n & T·∫£i file m·∫´u Email Mapping"):
                            st.info("ƒê·ªÉ g·ª≠i email, b·∫°n c·∫ßn t·∫£i l√™n file Excel ch·ª©a th√¥ng tin email c·ªßa c√°c ƒë∆°n v·ªã. B·∫°n c√≥ th·ªÉ t·∫£i file m·∫´u b√™n d∆∞·ªõi ƒë·ªÉ xem ƒë·ªãnh d·∫°ng.")
                            try:
                                with open("FileMau/Tong hop _ Report.xlsx", "rb") as file:
                                    st.download_button(
                                        label="üì• T·∫£i file m·∫´u (Tong hop _ Report.xlsx)",
                                        data=file,
                                        file_name="Tong hop _ Report.xlsx",
                                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                    )
                            except FileNotFoundError:
                                st.error("L·ªói: Kh√¥ng t√¨m th·∫•y file m·∫´u t·∫°i `FileMau/Tong hop _ Report.xlsx`.")
        
                        if 'credentials_json_content' not in st.session_state:
                            st.warning("Vui l√≤ng t·∫£i file `credentials.json` ·ªü tr√™n ƒë·ªÉ k√≠ch ho·∫°t ch·ª©c nƒÉng g·ª≠i email.")
                        else:
                            # --- SINGLE SEND ---
                            st.markdown("###### G·ª≠i cho 1 ƒë∆°n v·ªã")
                            unit_col = 'ƒê∆°n v·ªã'
                            unique_units_for_select_email = sorted(df_merged[unit_col].dropna().unique())
                            selected_unit_email = st.selectbox("Ch·ªçn ƒë∆°n v·ªã ƒë·ªÉ g·ª≠i email", unique_units_for_select_email, key="email_unit_select")
        
                            if selected_unit_email:
                                # Display emails for the selected unit if the map is loaded
                                if uploaded_email_mapping_file and unit_to_email_map_upload is not None:
                                    recipient_emails_display = unit_to_email_map_upload.get(selected_unit_email, [])
                                    if recipient_emails_display:
                                        st.info(f"B·∫£ng k√™ cho '{selected_unit_email}' s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒë·∫øn c√°c ƒë·ªãa ch·ªâ sau:")
                                        st.markdown(f"**{', '.join(recipient_emails_display)}**")
                                    else:
                                        st.warning(f"Kh√¥ng t√¨m th·∫•y ƒë·ªãa ch·ªâ email cho ƒë∆°n v·ªã '{selected_unit_email}' trong file ƒë√£ t·∫£i l√™n.")
        
                                if st.button(f"üìß G·ª≠i Email ƒë·∫øn '{selected_unit_email}'", use_container_width=True, key="send_email_btn"):
                                    if unit_to_email_map_upload is None:
                                        st.error("Vui l√≤ng t·∫£i l√™n file Email Mapping h·ª£p l·ªá tr∆∞·ªõc khi g·ª≠i.")
                                    else:
                                        recipient_emails = unit_to_email_map_upload.get(selected_unit_email, [])
                                        if not recipient_emails:
                                            st.error(f"Kh√¥ng th·ªÉ g·ª≠i email: Kh√¥ng t√¨m th·∫•y email cho ƒë∆°n v·ªã '{selected_unit_email}' trong file ƒë√£ t·∫£i l√™n.")
                                        else:
                                            to_field = ", ".join(recipient_emails)
                                            with st.spinner(f"ƒêang x√°c th·ª±c v√† g·ª≠i email ƒë·∫øn {to_field}..."):
                                                try:
                                                    creds, _ = get_google_credentials(st.session_state.credentials_json_content)
                                                    if not creds:
                                                        st.error("Kh√¥ng th·ªÉ l·∫•y th√¥ng tin ƒëƒÉng nh·∫≠p. Vui l√≤ng th·ª≠ ƒëƒÉng nh·∫≠p l·∫°i.")
                                                        st.stop()
                                                    df_unit = df_merged[df_merged[unit_col] == selected_unit_email]
                                                    
                                                    # 1. Create Excel attachment
                                                    excel_data_email = generate_bang_ke_excel(df_unit, df_unit['Date_dt'].min(), df_unit['Date_dt'].max(), date_col_name, pickup_col_name, dropoff_col_name)
                                                    safe_unit_name = "".join(c for c in str(selected_unit_email) if c.isalnum() or c in (' ', '_')).rstrip()
                                                    excel_filename = f"BangKe_{safe_unit_name}.xlsx"
                                                    attachments = [{'data': excel_data_email, 'filename': excel_filename}]
        
                                                    # 2. Create zipped PDF attachments for each employee
                                                    if 'pdf_content' in df_unit.columns:
                                                        df_unit_with_pdfs = df_unit[df_unit['pdf_content'].notna()]
                                                        if not df_unit_with_pdfs.empty:
                                                            employee_groups = df_unit_with_pdfs.groupby('Employee Name')
                                                            st.info(f"ƒêang t·∫°o v√† ƒë√≠nh k√®m {len(employee_groups)} file .zip (ch·ª©a PDF) cho t·ª´ng nh√¢n vi√™n...")
                                                            for employee_name, employee_df in employee_groups:
                                                                zip_buffer = io.BytesIO()
                                                                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_f:
                                                                    for _, row in employee_df.iterrows():
                                                                        if pd.notna(row['pdf_filename']) and pd.notna(row['pdf_content']):
                                                                            pdf_filename = os.path.basename(row['pdf_filename'])
                                                                            zip_f.writestr(pdf_filename, row['pdf_content'])
                                                                
                                                                safe_employee_name = "".join(c for c in str(employee_name) if c.isalnum() or c in (' ', '_')).rstrip()
                                                                zip_filename = f"{safe_employee_name}_pdf.zip"
                                                                
                                                                attachments.append({
                                                                    'data': zip_buffer.getvalue(),
                                                                    'filename': zip_filename
                                                                })
        
                                                    # 2.1. Create zipped XML attachments for each employee
                                                    if 'xml_content' in df_unit.columns:
                                                        df_unit_with_xmls = df_unit[df_unit['xml_content'].notna()]
                                                        if not df_unit_with_xmls.empty:
                                                            employee_groups_xml = df_unit_with_xmls.groupby('Employee Name')
                                                            st.info(f"ƒêang t·∫°o v√† ƒë√≠nh k√®m {len(employee_groups_xml)} file .zip (ch·ª©a XML) cho t·ª´ng nh√¢n vi√™n...")
                                                            for employee_name, employee_df in employee_groups_xml:
                                                                zip_buffer = io.BytesIO()
                                                                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_f:
                                                                    for _, row in employee_df.iterrows():
                                                                        if pd.notna(row['xml_filename']) and pd.notna(row['xml_content']):
                                                                            xml_filename = os.path.basename(row['xml_filename'])
                                                                            zip_f.writestr(xml_filename, row['xml_content'])
                                                                
                                                                safe_employee_name = "".join(c for c in str(employee_name) if c.isalnum() or c in (' ', '_')).rstrip()
                                                                zip_filename = f"{safe_employee_name}_xml.zip"
                                                                
                                                                attachments.append({
                                                                    'data': zip_buffer.getvalue(),
                                                                    'filename': zip_filename
                                                                })
        
                                                    # 3. Send email
                                                    subject = f"HOA DON GRAP"
                                                    body = f"K√≠nh g·ª≠i C∆° s·ªü {selected_unit_email},\n\nTrung t√¢m xin g·ª≠i h√≥a ƒë∆°n Grap ph√°t sinh trong k·ª≥. C√°n b·ªô thanh to√°n c∆° s·ªü vui l√≤ng xem c√°c file b·∫£ng k√™ v√† h√≥a ƒë∆°n (n·∫øu c√≥) ƒë∆∞·ª£c ƒë√≠nh k√®m trong email n√†y v√† th·ª±c hi·ªán h·ªì s∆° thanh to√°n ƒë√∫ng h·∫°n.\n\nM·ªçi th√¥ng tin th·∫Øc m·∫Øc, xin vui l√≤ng li√™n h·ªá: lientt3@fe.edu.vn\nƒê√¢y l√† h·ªá th·ªëng ƒë·ªëi chi·∫øu t·ª± ƒë·ªông, vui l√≤ng kh√¥ng reply email.\n\nTr√¢n tr·ªçng"
                                                    send_gmail_message(creds, to_field, subject, body, attachments)
                                                    st.success(f"‚úÖ ƒê√£ g·ª≠i email th√†nh c√¥ng ƒë·∫øn {to_field} cho ƒë∆°n v·ªã '{selected_unit_email}'.")
                                                except Exception as e:
                                                    st.error(f"L·ªói khi g·ª≠i email: {e}")
        
                            # --- BULK SEND ---
                            st.divider()
                            st.markdown("###### G·ª≠i cho t·∫•t c·∫£ c√°c ƒë∆°n v·ªã")
                            if st.button("üöÄ G·ª≠i Email cho T·∫§T C·∫¢ c√°c ƒë∆°n v·ªã", use_container_width=True, key="send_all_emails_btn"):
                                if unit_to_email_map_upload is None:
                                    st.error("Vui l√≤ng t·∫£i l√™n file Email Mapping h·ª£p l·ªá tr∆∞·ªõc khi g·ª≠i.")
                                else:
                                    with st.spinner("B·∫Øt ƒë·∫ßu qu√° tr√¨nh g·ª≠i email h√†ng lo·∫°t..."):
                                        creds, _ = get_google_credentials(st.session_state.credentials_json_content)
                                        if not creds:
                                            st.error("Kh√¥ng th·ªÉ l·∫•y th√¥ng tin ƒëƒÉng nh·∫≠p. Vui l√≤ng th·ª≠ ƒëƒÉng nh·∫≠p l·∫°i.")
                                            st.stop()
                                        units_to_email = sorted(df_merged[unit_col].dropna().unique())
                                        progress_bar = st.progress(0, text="B·∫Øt ƒë·∫ßu...")
                                        success_count = 0
                                        failed_units = []
                                        for i, unit in enumerate(units_to_email):
                                            progress_text = f"ƒêang x·ª≠ l√Ω: {unit} ({i+1}/{len(units_to_email)})"; progress_bar.progress((i + 1) / len(units_to_email), text=progress_text)
                                            recipient_emails = unit_to_email_map_upload.get(unit, [])
                                            if not recipient_emails:
                                                failed_units.append((unit, "Kh√¥ng t√¨m th·∫•y email trong file mapping ƒë√£ t·∫£i l√™n."))
                                                continue
                                            try:
                                                df_unit = df_merged[df_merged[unit_col] == unit]
                                                
                                                # 1. Create Excel attachment
                                                excel_data_email = generate_bang_ke_excel(df_unit, df_unit['Date_dt'].min(), df_unit['Date_dt'].max(), date_col_name, pickup_col_name, dropoff_col_name)
                                                safe_unit_name = "".join(c for c in str(unit) if c.isalnum() or c in (' ', '_')).rstrip()
                                                excel_filename = f"BangKe_{safe_unit_name}.xlsx"
                                                attachments = [{'data': excel_data_email, 'filename': excel_filename}]
        
                                                # 2. Create zipped PDF attachments for each employee in the unit
                                                if 'pdf_content' in df_unit.columns:
                                                    df_unit_with_pdfs = df_unit[df_unit['pdf_content'].notna()]
                                                    if not df_unit_with_pdfs.empty:
                                                        for employee_name, employee_df in df_unit_with_pdfs.groupby('Employee Name'):
                                                            zip_buffer = io.BytesIO()
                                                            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_f:
                                                                for _, row in employee_df.iterrows():
                                                                    if pd.notna(row['pdf_filename']) and pd.notna(row['pdf_content']):
                                                                        pdf_filename = os.path.basename(row['pdf_filename'])
                                                                        zip_f.writestr(pdf_filename, row['pdf_content'])
                                                            
                                                            safe_employee_name = "".join(c for c in str(employee_name) if c.isalnum() or c in (' ', '_')).rstrip()
                                                            zip_filename = f"{safe_employee_name}_pdf.zip"
                                                            
                                                            attachments.append({
                                                                'data': zip_buffer.getvalue(),
                                                                'filename': zip_filename
                                                            })
        
                                                # 2.1. Create zipped XML attachments for each employee
                                                if 'xml_content' in df_unit.columns:
                                                    df_unit_with_xmls = df_unit[df_unit['xml_content'].notna()]
                                                    if not df_unit_with_xmls.empty:
                                                        for employee_name, employee_df in df_unit_with_xmls.groupby('Employee Name'):
                                                            zip_buffer = io.BytesIO()
                                                            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_f:
                                                                for _, row in employee_df.iterrows():
                                                                    if pd.notna(row['xml_filename']) and pd.notna(row['xml_content']):
                                                                        xml_filename = os.path.basename(row['xml_filename'])
                                                                        zip_f.writestr(xml_filename, row['xml_content'])
                                                            
                                                            safe_employee_name = "".join(c for c in str(employee_name) if c.isalnum() or c in (' ', '_')).rstrip()
                                                            zip_filename = f"{safe_employee_name}_xml.zip"
                                                            
                                                            attachments.append({
                                                                'data': zip_buffer.getvalue(),
                                                                'filename': zip_filename
                                                            })
        
                                                # 3. Send email
                                                subject = f"B·∫£ng k√™ ƒë·ªëi chi·∫øu Grab cho ƒë∆°n v·ªã '{unit}'"
                                                body = f"K√≠nh g·ª≠i Qu√Ω ƒë∆°n v·ªã {unit},\n\nVui l√≤ng xem c√°c file b·∫£ng k√™ v√† h√≥a ƒë∆°n (n·∫øu c√≥) ƒë∆∞·ª£c ƒë√≠nh k√®m trong email n√†y.\n\nTr√¢n tr·ªçng,\nSerder mail."
                                                to_field = ", ".join(recipient_emails)
                                                send_gmail_message(creds, to_field, subject, body, attachments)
                                                success_count += 1
                                            except Exception as e:
                                                failed_units.append((unit, str(e)))
                                        progress_bar.empty()
                                        st.success(f"‚úÖ Ho√†n t·∫•t! ƒê√£ g·ª≠i th√†nh c√¥ng {success_count}/{len(units_to_email)} email.")
                                        if failed_units:
                                            st.error(f"‚ùå C√≥ {len(failed_units)} email g·ª≠i th·∫•t b·∫°i.")
                                            with st.expander("Xem chi ti·∫øt l·ªói"):
                                                for unit, reason in failed_units: st.write(f"- **{unit}**: {reason}")
                except Exception as e:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói khi t·∫°o file B·∫£ng k√™ ho·∫∑c g·ª≠i mail: {e}")

        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
            st.exception(e)

# --- H√ÄM H·ªñ TR·ª¢ ---
def find_col(df, possibilities):
    """Finds the first column in a dataframe that exists from a list of possibilities."""
    for p in possibilities:
        if p in df.columns:
            return p
    return None

def load_mapping_data():
    """ƒê·ªçc file Excel mapping v√† tr·∫£ v·ªÅ 2 dictionaries:
    1. Employee Name -> ƒê∆°n v·ªã
    2. ƒê∆°n v·ªã -> List of Emails
    """
    try:
        df_mapping = pd.read_excel("FileMau/Tong hop _ Report.xlsx")
        name_col = df_mapping.columns[1]
        email_col = df_mapping.columns[3]
        unit_col = df_mapping.columns[4]
        df_mapping = df_mapping.dropna(subset=[name_col, unit_col])
        employee_to_unit_map = df_mapping.set_index(name_col)[unit_col].to_dict()
        df_email_map = df_mapping.dropna(subset=[email_col])
        # Group by unit and create a list of unique emails for each unit
        unit_to_email_map = df_email_map.groupby(unit_col)[email_col].apply(lambda x: list(x.unique())).to_dict()
        return employee_to_unit_map, unit_to_email_map
    except FileNotFoundError:
        st.error("L·ªói: Kh√¥ng t√¨m th·∫•y file mapping 'FileMau/Tong hop _ Report.xlsx'. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
        st.stop()
    except IndexError:
        st.error("L·ªói: File mapping 'Tong hop _ Report.xlsx' kh√¥ng c√≥ ƒë·ªß 5 c·ªôt (ƒë·ªÉ l·∫•y c·ªôt B, D, v√† E).")
        st.stop()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file mapping: {e}")
        st.stop()

def send_gmail_message(credentials, to, subject, body, attachments=None):
    """Sends an email with multiple attachments using Gmail API."""
    try:
        # L·∫•y th√¥ng tin credentials t·ª´ session state
        creds_json = json.loads(st.session_state['credentials'])
        creds = Credentials.from_authorized_user_info(creds_json, SCOPES)
        
        service = build('gmail', 'v1', credentials=creds)
        user_profile = service.users().getProfile(userId='me').execute()
        sender_email = user_profile['emailAddress']
        sender_name = "H·ªá th·ªëng ƒë·ªëi chi·∫øu t·ª± ƒë·ªông"
        
        message = MIMEMultipart()
        message['to'] = to
        message['from'] = formataddr((sender_name, sender_email))
        message['subject'] = subject
        msg_body = MIMEText(body, 'plain', 'utf-8')
        message.attach(msg_body)

        if attachments:
            for attachment in attachments:
                if attachment and attachment.get('data') and attachment.get('filename'):
                    part = MIMEApplication(attachment['data'], Name=attachment['filename'])
                    part['Content-Disposition'] = f'attachment; filename="{attachment["filename"]}"'
                    message.attach(part)

        encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
        create_message = {'raw': encoded_message}
        send_message = (service.users().messages().send(userId="me", body=create_message).execute())
    except HttpError as error:
        st.error(f"An error occurred while sending email: {error}")
        raise error
    except KeyError:
        st.error("L·ªói: Kh√¥ng t√¨m th·∫•y th√¥ng tin ƒëƒÉng nh·∫≠p trong session. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.")
        st.stop()

# --- ƒêI·ªÇM B·∫ÆT ƒê·∫¶U C·ª¶A APP ---
if 'user_info' in st.session_state:
    main_app()
else:
    show_login_page()